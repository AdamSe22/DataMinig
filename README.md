# ğŸ“ PrÃ©diction des Performances des Ã‰tudiants â€” RÃ©gression Multiple

## ğŸ“˜ Description du Projet
Ce projet vise Ã  **analyser les facteurs influenÃ§ant la performance scolaire des Ã©tudiants** et Ã  **prÃ©dire leurs rÃ©sultats finaux** Ã  lâ€™aide de plusieurs modÃ¨les de rÃ©gression supervisÃ©e.  
Lâ€™objectif est dâ€™identifier les variables les plus dÃ©terminantes pour la rÃ©ussite acadÃ©mique et dâ€™utiliser ces informations pour anticiper les performances futures.

---

## ğŸ¯ Objectifs du Projet
- Identifier les variables ayant le plus grand impact sur la rÃ©ussite scolaire.  
- Construire et comparer plusieurs modÃ¨les de rÃ©gression pour prÃ©dire la performance.  
- Visualiser et interprÃ©ter les rÃ©sultats Ã  lâ€™aide de graphiques et dâ€™analyses statistiques.

---

## ğŸ§  ProblÃ©matique de RÃ©gression
**Question principale :**  
> Quels sont les facteurs clÃ©s influenÃ§ant les performances scolaires des Ã©tudiants, et comment peuvent-ils Ãªtre utilisÃ©s pour prÃ©dire leurs rÃ©sultats finaux ?

---

## ğŸ“Š Jeu de DonnÃ©es
**Fichier :** `Student_Performance.csv`  
**Taille :** 9873 lignes Ã— 6 colonnes  

| Variable | Description |
|-----------|-------------|
| Hours Studied | Nombre d'heures dâ€™Ã©tude par jour |
| Previous Scores | Notes prÃ©cÃ©dentes |
| Extracurricular Activities | Participation Ã  des activitÃ©s extrascolaires (Yes/No) |
| Sleep Hours | Heures de sommeil |
| Sample Question Papers Practiced | Nombre dâ€™examens dâ€™entraÃ®nement pratiquÃ©s |
| Performance Index | Note finale (variable cible) |

---

## âš™ï¸ PrÃ©traitement des DonnÃ©es
- **Suppression des doublons** â†’ 127 lignes supprimÃ©es  
- **Encodage des variables catÃ©gorielles** (`Extracurricular Activities` â†’ `activities`)  
- **DÃ©tection et suppression des valeurs aberrantes** Ã  lâ€™aide du **Z-score**  
- **Analyse de la distribution** des variables et de leur **corrÃ©lation** avec la cible  

ğŸ“Š **Exemples de visualisations :**
- Heatmap des corrÃ©lations  
- Boxplots et distributions  
- Pairplot entre les variables  

---

## ğŸ§© ModÃ¨les UtilisÃ©s

| ModÃ¨le | RÂ² Score | MAE |
|--------|-----------|------|
| Linear Regression | 0.9880 | 1.66 |
| Ridge Regression | 0.9880 | 1.66 |
| Lasso Regression | 0.9860 | 1.80 |
| Decision Tree Regressor | 0.9739 | 2.46 |
| Random Forest Regressor | 0.9840 | 1.93 |
| Gradient Boosting Regressor | 0.9873 | 1.71 |
| SVR | 0.9847 | 1.87 |
| KNN Regressor | 0.9831 | 1.99 |
| XGBoost Regressor | 0.9857 | 1.81 |

ğŸ“ˆ **Meilleur modÃ¨le : Random Forest Regressor**

---

## ğŸ” Validation et Optimisation

### ğŸ”¸ Validation croisÃ©e (Cross-Validation)
| ModÃ¨le | RÂ² moyen |
|---------|-----------|
| Ridge Regression | 0.9888 |
| Lasso Regression | 0.9887 |
| Random Forest | 0.9850 |

### ğŸ”¸ Optimisation via GridSearchCV
**ParamÃ¨tres optimaux trouvÃ©s :**
```python
{'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150}
```

### ğŸ”¸ RÃ©sultats aprÃ¨s optimisation :
- **RÂ² optimisÃ© :** 0.9859  
- **MAE optimisÃ© :** 1.8166  

---

## ğŸ§® RÃ©duction de DimensionnalitÃ© (PCA)
| Composante | Variance expliquÃ©e |
|-------------|-------------------|
| PC1 | 94.34% |
| PC2 | 2.56% |
| PC3 | 2.11% |
| PC4 | 0.90% |
| PC5 | 0.07% |

ğŸ“ˆ Le modÃ¨le **Random Forest + PCA** obtient :
- **RÂ² :** 0.9847  
- **MAE :** 1.9006  

---

## ğŸ“‰ Visualisations des RÃ©sultats
- **Comparaison des ModÃ¨les** (Barres des scores RÂ² et MAE)  
- **Courbe dâ€™Apprentissage** (Random Forest Regressor)  
- **Distribution des RÃ©sidus**  
- **RÃ©sidus vs PrÃ©dictions**

---

## ğŸ§¾ BibliothÃ¨ques UtilisÃ©es
- **pandas**
- **numpy**
- **matplotlib**
- **seaborn**
- **scikit-learn**
- **xgboost**

---

## ğŸ‘¨â€ğŸ’» Auteur

**Adam Serghini**  
IngÃ©nieur passionnÃ© par la **Data Science** et lâ€™**Intelligence Artificielle**.  
Titulaire dâ€™un **Master 2 en Intelligence Artificielle (IA)**.  


ğŸ“§ [LinkedIn](https://www.linkedin.com/in/adam-serghini-767b47273)

---

## ğŸ“š Licence
Ce projet est sous **licence MIT** â€” libre Ã  lâ€™utilisation, la modification et la diffusion.
